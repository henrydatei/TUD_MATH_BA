\chapter{Introduction}
\section*{The central issue of financial mathematics}
\subsection*{\begriff{Valuation}:}
Valuation of derivatives and \emph{hedge} against the risks which emerges from the purchase / sale,

\begin{*definition}[\begriff{Derivative}]
	Financial product whose payouts are derived from price of one or more \begriff{basic goods} derived (underlying) derivative.
\end{*definition}

\begin{*example}
	\begin{itemize}
		\item Right to get 100,000 GBP in 3 months against 125,000 EUR (\begriff{Call-Option}, Underlying: Exchange rate GBP/EUR)
		\item Right within the next year to consume 100,000 Mw / h of electric energy at the price of 30EUR/Mwh with minimum order quantity of 50,000 Mwh (\begriff{Swing-Option}, Underlying: electricity price)
		\item buying and selling options on stock (Underlying: equity price)
	\end{itemize}
\end{*example}
Issue:  What is the ''fair'' price for such a derivative? (``Pricing''). How can the sellers protect themselves against the … risks? (``Hedging'')

\subsection*{\begriff{Optimal investment}}
Gathering Porftolios that are optimal for risk-return approach.
\begin{itemize}
	\item How do I weigh risk against profit?
	\item What exactly is ''optimal''?
	\item Solution of the resulting optimization problems
\end{itemize}
\subsection*{\begriff{Risk management + Risk measurement}}
\begin{itemize}
	
	\item Legal rules (Basel + Solvency) should ensure stability of the banking system/insurance system even in the face of various risks 
	
	$\implies$ Mathematical theory of convex + coherent risk measures 
\end{itemize}	 
Mathematical tools: Probability Theory + stochastic processes (dynamics in time, some linear algebra, optimization, measure theory).

\section*{Mathematical Financial Market Model}
We consider:
\begin{enumerate}
	\item \emph{Probability space} $(\O,F,P)$, later more probability measures $Q, \dots$ on the same measure space $(\O,F), \omega \in \O$ basic events or ''scenarios''.
	\item \emph{Timeline} $I$ is either $I=\set{t_0, t_1, \dots, t_N=T}$ $N$-period model (discrete model) or $I = [0,T]$ (continious-time model), where $T = $ time-horizon\\
	A \begriff{stochastic process} $S$ is a measurable mapping $S: (\O,\F) \to \R^d \mit (\omega, t) \mapsto S_t(\omega)$\\
	Especially:
	\begin{itemize}
		\item $t \mapsto S_t(\omega)$ function $I \to R^d$ for every $\omega \in \O$ (``path'')
		\item $\omega \mapsto S_t(\omega)$ random variable $\O \to \R^d$ for every $t \in I$
	\end{itemize}
	\item \emph{Percolation} 
	a sequence of $\omega$-algebras $(\F_t)_{t \in I}$ with the property $\F_S \subseteq \F_t \quad \forall s,t \in I, x \le t$ and $\F_t \subseteq F\quad \forall t \in I$\\
	Interpretation: $F_t=$ market participant at time $t$ known/available information\\
	Events $A \in F_t$ are considered known 'at time t'\\
	A $R^d$-valued RV $X$ is called  \begriff{$F_t$-measurable}, if $E = X^{-1}(B) \in F_t \quad \forall$ Borel sets $B \subseteq R^d$ ($E$ is actually the preimage of $B$).
	\begin{*example}
		A stochastic process  $(S_t)_{t\in I}$ on $(\O,F)$ is called \begriff[stochastic process]{adaptiert} regarding a percolation $(\F_t)_{t \in I}$, wenn gilt:
		\begin{align*}
			S_t \text{ is } F_t-\text{measurable} \quad \forall t \in I
		\end{align*}
	\end{*example}
	Interpretation: ``the value $S_t$ is known at time $t$''\\
	Why percolation in the financial mathematics (FiMa)?
	\begin{itemize}
		\item Differentiation between future/past
		\item Different information (Insider/Outsider) corresponds to different percolation $(F_t)_{t \in I}$ or $(G_t)_{t\in I}$
	\end{itemize}
	$S^i$= price of the i-th asset at the time t
	\item \begriff{Assets} $R^{d+1}$-valued  stochastic process with components
	\begin{align*}
		S^i: (\O \times I) \to \R\quad (\omega,t) \mapsto S^i_t(\omega) \mit i \in \set{0,1,\dots,d}
	\end{align*} 
	where $S^i_t=$ price of the $i$-th asset at time $t$\\
	$S^i, i \in \set{1,\dots,d}$ is typically
	\begin{itemize}
		\item Stock, company share
		\item Currency or exchange rate
		\item Ccommodity such as oil, noble metal, electricity,..
		\item Bond ... 
	\end{itemize}
	Principal assumption: $S^i$ is liquid traded (eg on exchange), ie purchase/sale for the price $S_t^i$ possible at any time.\\
	
	$S^0\dots$ ''numeraire'' has a special role: describes interest rate of \emph{not} in $(S^1,..,S^d)$ invested capital; is mostly considered to be \emph{risk-free}.
\end{enumerate}

\begin{definition}[Finance market model]
	A \begriff{finance market model} (FMM) with a time axis $I$ is given by
	\begin{enumerate}
		\item a probability space $(\O, F,P)$ with percolation $(F_t)_{t\in I}$
		\item an adapted to $(F_t)_{t \in I}$, $R^{d+1}$-valued stochastic process $S_t = (S^0_t, S_t^1, \dots, S^d_t),t \in I$
	\end{enumerate} 
\end{definition}

\begin{*example}[\person{Cox}-\person{Rubinstein} (CRR)-model (discrete-time)]
	\begin{itemize}
		\item $S^0_n = (1+r)^n$, meaning interest at a constant rate $r$
		\item $S^1_n = S_0^1 \prod_{k=1}^n(1+Ru)$, wobei $(R_1, R_2, \dots)$ independent random variables with two possible values $a < b$\\
		Image: '' ``recombined tree '' with events $\omega$ corresponding paths in the tree
	\end{itemize}
\end{*example}
\begin{*example}[\person{Block}-\person{Scholes}-modell (continious-time)]
	\begin{itemize}
		\item $S^0_t = e^{rt}$, meaning interest at a constant rate $r$
		\item $S_t^1 = S_0^1\cdot \exp((\mu - \frac{\sigma^2}{2}t + \sigma\beta_t) \mit \mu \in \R, \sigma > 0, S^1_0 >0$ und $\beta_t$ 
		corresponds to Brownian motion(stochastic process in continuous time) and $\mu - \frac{\omega^2}{2}$ corresponds to trend component
	\end{itemize}
	Image: Exchange curve = $S_t(\omega)$, wherein time-continuous model for infinite probability space
\end{*example}
%TODO add 2nd lecture
\section{Conditional expectation values and Martingale} %1.4
\subsection{Conditional density and conditional expected value}
Motivation: Given: Two random variables $(X,Y)$ with values in $\R^m \times \R^n$ and joint density $f_{XY}(x,y)$. From $f_{XY}$ we can derive:
\begin{itemize}
	\item $f_{Y}(y) := \int_{\R^m} f_{XY}(x,y) \d x$ with marginal distribution of $Y$
	\item $S_Y := \set{y \in \Rn \colon f_Y(y) > 0}$ carrier of $Y$ - Image?
\end{itemize}
\begin{*definition}[Conditional density of $X$ with respect to $Y$]
	Conditional density from $X$ with respect to $Y$ is defined as
	\begin{align*}
		f_{X\mid Y}(x,y) = \begin{cases}
			\frac{f_{XY}(x,y)}{f_Y(y)} &\quad y \in S_Y\\
			0 &\quad y\notin S_Y
		\end{cases}
	\end{align*}
\end{*definition}
Consider the following problem:\\
What is the best forecast from $X$ if an observation $Y = y$ is given?\\
Criteria:\\
Minimiye the quadratic distance/second moment/ $L_2$-norm.\\
Vorhersage:\\
Measurable function $g: \Rn \to \R^m \mit y \mapsto g(y)$, meaning,.
\begin{align*}
	\min\set{\E[(X-g(Y))]^2 \colon g \text{ messbar } \R^n \to \R^m} \tag{min-1}\label{eq_min_1}
\end{align*}
\begin{proposition} %1.3
	When $(X,Y)$ have a joint density with $\E[\abs{X}^2] < \infty$, then \eqref{eq_min_1} is going to be minimized through the conditional expected value
	\begin{align*}
		g(y) = \E[X\mid Y=y] := \int_{\R^m} x f_{X\mid Y}(x,y)\d x
	\end{align*}
	(where $\E[X\mid Y=y]$ ``expected value of $X$ conditioned on $Y=y$'')
\end{proposition}
In general, it holds:
\begin{theorem} %1.4
	Let $(X,Y)$ be random variables with joint density on$\R^m \times \Rn$, $h: \R^m \to \R^n$ measurable with $\E[h(X,y)^2]$. Then that is going to be minimisation problem
	\begin{align*}
		\min\set{\E[(h(X,Y) - g(y))^2]} \quad g\text{measurable from $\Rn$ towards $\R$}
		\intertext{solved through}
		g(y) = \E[h(X,Y) \mid Y=y] = \int_{\R^m} h(X,Y)f_{X\mid Y}(x,y) \d x
	\end{align*}
\end{theorem}
\begin{proof}[only proposition, the Theorem is analogous, for $n=1$]
	Set $g(y) = \int_{\R} f_{X\mid Y}(x,y) \d x$. Sei $p: \R \to \R$ arbitrary measurable function with $\E[p(y)^2] < \infty$. Set $g_{\epsilon}(y) = g(y) + \epsilon p(y)$. Minimize
	\begin{align*}
		F(\epsilon) &:= \E[(X-g_{\epsilon}(y))^2] = \E[(X-g(y)-\epsilon p(y))^2]\\
		&= \E[(X-g(y))^2] - 2\epsilon\E[(X-g(y))p(y)] + \epsilon^2\E[p(y)^2]\\
		\frac{\partial F}{\partial \epsilon}(\epsilon) &= 2 \epsilon \E[p(y)^2] - 2\E[(X-g(y))p(y)]\\
		&\implies \epsilon_{\ast} :=\frac{\E[(X-g(y))p(y)]}{\E[p(y)^2]} = \frac{A}{B}
		\intertext{wobei}
		A&= \E[Xp(y)] -\E[g(y)p(y)] \\
		&= \int_{\R \times \Rn} xp(y)f_{XY}(x,y)\d x \d y - \int_{S_y}g(y)p(y)f_Y(y) = [\text{Einsetzen von $g$ + Fubini}]\\
		&= \int_{\R \times \Rn} xp(y)f_{XY}(x,y)\d x \d y - \int_{\R\times S_y} xp(y)\underbrace{f_{X\mid Y}(x,y)f_Y(y\d y)}_{=f_{XY}(x,y)} = 0
	\end{align*}
	so $\epsilon^{\ast} = 0$ independent from  $p$ $\implies g(y)$ minimizes \eqref{eq_min_1}.
\end{proof}
\begin{*example}
	Let $(X,Y)$ normaly distributed on $\R\times \R$ with
	\begin{align*}
		\mu = (\mu_x, \mu_y)^T \quad \Sigma = \begin{pmatrix}
			\sigma x^2 \rho\sigma_x \sigma_y\\
			\rho \sigma_x \sigma_y & \sigma_y^2
		\end{pmatrix} = \begin{pmatrix}
			\Var(X) & \Cov(X,Y)\\
			\Cov(X,Y) & \Var(Y)
		\end{pmatrix} \mit \rho \in [-1,1]
	\end{align*}
	Then the arbitraty density is $f_{X\mid Y}(x,y)$. ($\Sigma$ covariance matrix). Once more the density of a normaly distributed random variable with 
	\begin{align*}
		\E[X \mid Y=y] &= \mu_x + \rho \frac{\sigma_x}{\sigma_y}(y-\mu_y)\\
		\Var(X\mid Y=y) &= \sigma_x^2(1-\rho^2)
	\end{align*}
	(is ÜA!). The mapping $y \mapsto \mu_x + g(y)\frac{\sigma_x}{\sigma_y}(y-\mu_y)$ is called regression line for $X$ given $Y=y$.\\
	Image: $\mu_x,\mu_y$ are values on $x,y$-axis and the $\sigma$'s build the Triangle slope (slope is known substantially by $\rho$)\\
	For disrete random variables, i.e. when $X,Y$ accept only finitely many $\set{x_1,\dots,x_m}$ or $\set{y_1,\dots,y_m}$ annehmen then with similar considerations we obtain as a solution of \eqref{eq_min_1}
	\begin{align*}
		\E[X\mid Y=y_j] = \sum_{i=1}^m X_i \P(X=x_i \mid Y=y_j)
	\end{align*}
	wherein directly the conditional probabilities
	\begin{align*}
		\P(X=x_i \mid Y=y_j) = \begin{cases}
			\frac{\P(X=x_i \wedge Y=y_j)}{\P(Y=y_j)} &\quad \text{ wenn } \P(Y=y_j) > 0\\
			0 &\quad \text{ wenn } \P(Y=y_j) = 0 
		\end{cases}
	\end{align*}
\end{*example}
\subsection{Conditional expectation - measure theoretical access}
We consider a probability space $(\O, F,P)$. For random variables $X: \O \to \R$ und $p \in [1,\infty)$ we define the $L_p$-norm
\begin{align*}
	\norm{X}_p = \E[\abs{X}^p]^{1/p} = \brackets{\int_{\O} \abs{X(\omega)}^p \d \P(\omega)}^{1/p}
\end{align*}
and $L_p$-space $L_p(\O,F,P):= \set{X: \O \to \R\colon \F-\text{measurable}, \norm{X}_p < \infty}$. We identificate random variables which differ only at zero amounts, ie $\P(X \neq X') = 0 \implies X = X'$ (in $L_p$).\\
From measure theory it is known: (?)\\
The spaces $L_p(\O,F,P)$ with norm $\norm{\cdot}_p, p \in [1,\infty)$ are always \person{Banach}-spaces (linear, complete, normed vector spaces). For $p = 2$ also hilbert spaces with inner product
\begin{align*}
	\scaProd{X}{Y} = \E[XY] = \int_{\O} X(\omega)Y(\omega)\d \P(\omega)
\end{align*}
Für $\G \subseteq \F$ Unter-$\sigma$-slgebra is $L_p(\O,\F,\P) \subseteq L_p(\O,\F,\P)$ closed subspace.\\
We generalize ''prediction problem'' from the last section (1.3?)\\
Given are random variables $X$ from $L_2(\O,\F,\P)$ is $\G \subseteq \F$ Sub-$\sigma$-algebra.\\
What is the best $\G$-measurable forecast for $Y$?
\begin{align*}
	\min\set{\E[(X-G)^2] \colon G \in L_2(\O,\F,\P)} \tag{min-2}\label{eq_min_2}
\end{align*}
wobei $\E[(X-G)^2] = \norm{X-G}^2_2$.\\
From hilbert-space theory:\\
\eqref{eq_min_2} possesses a unique solution $G_{\ast} \in L_2(\F,\G,\P)$. $G_{\ast}$ is optimization (with respect to $\scaProd{\cdot}{\cdot}$) from $X \in L_2(\O,F,P)$ on closed subspace $L_2(\O,G,P)$\\
Image: maybe from Eric (Orthogonal projection on the subspace)\\
We denote the conditioned expected value $\E[X\mid \G]$ of $X$ with recpect to $\G$ with $G_{\ast}$.
\begin{theorem} %1.5
	Let $X,Y \in L_2(\O,\F,\P)$ und $\G \subseteq F$ Unter-$\sigma$-Algebra. Dann gilt
	\begin{enumerate}
		\item (Linearität) $\E[aX+bY] = a\E[X\mid \G] + b\E[Y\mid \G]$
		\item (Turmregel) Für jede weitere $\sigma$-Algebra $\H \subseteq\G$ gilt
		\begin{align*}
			\E[E[X\mid \G \mid \H]] = \E[X\mid \H]
		\end{align*}
		\item (Pullout-Property) $\E[XZ\mid \G] = Z\E[X\mid \G]$, wenn $Z$ beschränkt und $\G$-messbar ist.
		\item (Monotonie) $X \le Y \implies \E[X\mid \G] \le \E[Y \mid \G]$
		\item ($\Delta$-Ungleichung) $\abs{\E[X\mid \G]} \le \E[\abs{X}\mid \G]$
		\item (Unabhängigkeit) $X$ unabhängig von $G$ $\implies$ $\E[X \mid \G] = \E[X]$
		\item (triviale $\sigma$-Algebra) $\G=\set{\emptyset, \O} \implies \E[X \mid \G] = \E[X]$ 
	\end{enumerate}
\end{theorem}
\begin{proof}
	(ohne Beweis, siehe VL W-Theorie mit Martingalen oder auch STOCH-Skript SS19.)
\end{proof}
\begin{*remark}
	\begin{itemize}
		\item Die für $X \in L_2(\O,\F,\P)$ definierte vedingte Erwartung $\E[X\mid \G]$ lässt sich durch Approximation auf alle $X\in L_1(\O,\F,\P)$ erweitern. Alle Eigenschaften aus Theorem \propref{1_5_eigen_bedEW} bleiben erhalten!
		\item Sei $Y$ eine ZVe und $\G = \sigma(Y)$ die von $Y$ erzeugte $\sigma$-Algebra. Wir schreiben:
		\begin{align*}
		\E[X\mid Y] = \E[X \mid \sigma(Y)] \quad \sigma\text{-messbare ZVe}
		\end{align*}
		\item Maßtheorie: \person{Doob}-\person{Dynkin}-Lemma $\implies \exists$ messbare Funktion $g: \Rn \to \R$ sodass
		\begin{align*}
		\E[X\mid Y] = g(Y)
		\end{align*}
		Dabei ist $g$ genau die Funktion aus \eqref{eq_min_1}.
	\end{itemize}
\end{*remark}
Zusammenfassung:\\
Sei $X,Y$ aus $L_1(\O,\F,\R)$, $\G \subseteq \F$ Unter-$\sigma$-Algebra
\begin{enumerate}
	\item $\E[X\mid Y=y]$ ist messbare Funktion $g: \Rn \to \Rn$. Falls bedingte Dichte existiert, gilt:
	\begin{align*}
		\E[X\mid Y=y] = \int_{\R^m} f_{X\mid Y} (x,y) \d x
	\end{align*}
	\item $\E[X\mid Y]$ ist eine $\sigma(y)$-messbare ZVe, diese kann als $g(Y)$ dargestellt werden. Falls bedingte Dichte existiert, gilt
	\begin{align*}
		\E[X\mid Y](\omega) = \int_{\Rn}xf_{X\mid Y}(x,Y(\omega))\d x
	\end{align*}
	\item $\E[X \mid \G]$ ist eine $\G$-messbare ZVe. Falls $\G = \sigma(y)$ tritt 2) ein.
\end{enumerate}
In allgemeinen Fall kann $\bar{\E[X\mid \cdot]}$ interpretiert werden als \emph{beste Vorhersage} für $X$, gegeben
\begin{enumerate}
	\item punktweise Beobachtung $Y=y$
	\item Beobachtung $Y$
	\item Information $\G$
\end{enumerate}
\subsection{Martingale}
Prototyp eines ``neutralen'' stochastischen Prozesses,der weder Aufwärts- noch Abwärtstrend besitzt. Hier nur in diskrete Zeit $Z = \N_0$.
\begin{*definition}[Martingal ohne Filtration]
	Sei $(X_n)_{n\in \N_0}$ stochastischer Prozess. Wenn gilt
	\begin{enumerate}
		\item $\E[\abs{X_n}] < \infty$ $\forall n \in \N$
		\item $\E[X_{n+1},\dots, X_n] = X_n$ $\forall n \in \N$
	\end{enumerate}
	dann heißt $(X_n)$ \begriff{Martingal}. Wen wir $\F_n^{\ast} = \sigma(X_1,\dots,X_n)$ definieren, können wir 2) schreiben als
	\begin{align*}
		\E[X_{n+1} \mid \F_n^{\ast}] = X_n \quad \forall n \in \N
	\end{align*}
\end{*definition}
Interpretation:\\
\begin{itemize}
	\item Beste Vorhersage für zukünftigen Wert $X_{n+1}$, basierend auf Vergangenheit $\sigma(X_1,\dots,X_n)$ ist der momentane Wert $X_n$.
	\item Aus der Turmregel folgt
	\begin{align*}
		\E[X_{n+k} \mid \F_n^{\ast}] &= X_n \quad n,k \in \N_0
		\intertext{denn}
		\E[X_{n+k}\mid\F_n^{\ast}] &= \E[\E[X_{n+k}\mid \F_{n+k-1}\mid \F_n^{\ast}]] = \E[X_{n+k-1}\mid \F_n^{\ast}] = (k\text{-mal}) = X_n
	\end{align*}
\end{itemize}
Kann von $(\F_{n})_{n \in \N}$ auf beliebige Filtrationen $(\F_n)_{n \in \N_0}$ erweitert werden.
\begin{*definition}[Martingal mit Filtration]
	Sei $(X_n)_{n \in \N_0}$ ein stochastischer Prozess, adaptiert an eine Filtration $(\F_n)_{n \in \N_0}$. Wenn gilt
	\begin{enumerate}
		\item $\E[\abs{X_n}] < \infty$ $\forall n \in \N_0$
		\item $\E[X_{n+1} \mid \F_n] = X_n$ $\forall n \in \N_0$
	\end{enumerate}
	dann heißt $(X_n)_{n \in \N_0}$ \begriff{Martingal bezüglich Filtration} $(\F_n)_{n \in \N_0}$
\end{*definition}
Interpretation:\\
Beste Vorhersage für zukünftige Werte $X_{n+1}$, basierend auf verfügbarer Information $\F_n$ ist momentane Wert $X_n$.
\begin{*definition}[Supermartingal, Submartingal]
	Falls in Punkt 2) statt ``$=$'' die Ungleichung $\le \oder \ge$ gilt, so heißt $(X_n)_{n \in \N}$ \begriff{Supermartingal} bzw. \begriff{Submartingal}.
\end{*definition}
Erste Beobachtung:\\
\begin{itemize}
	\item $X$ Martingal $\implies \E[X_n] = X_0$, d.h. $n \mapsto \E[X_n]$ ist konstant.\\
	Begründung:
	\begin{align*}
		\E[X_{n+1} \mid \F_n] = X_n \implies \E[\E[X_{n+1}\mid \F_n]] = \E[X_n] = \E[X_{n+1}] \implies (n\text{-mal Anwendung } \E[X_n] = X_0)
	\end{align*}
	Bild: Erwartungswert konstant, aber kein Martingal.
	\item $X$ Submartingal $\implies n \mapsto \E[X_n]$ ist monoton steigend
	\item $X$ Supermartingal $\implies n \mapsto \E[X_n]$ ist monoton fallend
\end{itemize}
Um sich den Unterschied zwischen Super- und Submartingal zu merken, hier eine kleine Hilfe:\\
``Das leben ist ein Supermartingal, die Erwartungen fallen mit der Zeit.''
\begin{*example}
	\begin{itemize}
		\item Seien $(Y_n)_{n\in \N}$ unabhängige ZVen in $L_1(\O,\F,\P)$ mit $\E[Y_n] = 0$. Definiere $X_n := \sum_{k=1}^n Y_k \mit X_0 = 0$. Dann ist $(X_n)_{n \in \N_0}$ Martingal, denn
		\begin{enumerate}
			\item $\E[\abs{X_n}] \le \sum_{k=1}^n \E[\abs{Y_k}] < \infty \quad \forall n \in \N$ \checkmark
			\item
			\begin{align*}
				\E[X_{n+1} \mid \F_n^{\ast}] &= \E[Y_{n+1} + X_n \mid \F_n^{\ast}]\\
				&= \E[Y_{n+1} \mid \F_n^{\ast}] = \E[X_n \mid \F_n^{\ast}] \quad (\text{ Turm und $\F_n^{\ast}$-messbar})\\
				&= \underbrace{\E[Y_{n+1}]}_{=0} + X_n = X_n \checkmark
			\end{align*}
		\end{enumerate}
		\item weitere Beispiele auf dem ersten Übungsblatt!
	\end{itemize}
\end{*example}
\begin{*definition}[vorhersehbar]
	Sei $(\F_n)_{n\in \N_0}$ eine Filtration. Ein stochastischer Prozess $(X_n)_{n \in \N}$ heißt \begriff{vorhersehbar} (predictable) bezüglich $(\F_n)_{n \in \N_0}$, wenn gilt:
	\begin{align*}
		H_n \text{ ist } \F_{n-1}\text{-messbar} \quad \forall n \in \N
	\end{align*}
\end{*definition}
\begin{*remark}
	Stärkere Eigenschaft als ``adaptiert''.
\end{*remark}
\begin{*definition}[diskretes stochastische Integral]
	Sei $X$ adaptierter und $H$ ein vorhersehbarer stochastischer Prozess bezüglich $(\F_n)_{n \in \N}$. Dann heißt
	\begin{align*}
		(H \bigcdot X)_n := \sum_{k=1}^n H_k (X_k - X_{k-1}) \tag{$\ast$}\label{eq_pred_stoch_process}
	\end{align*}
	\begriff{diskretes stochastische Integral} von $H$ bezüglich $X$.
\end{*definition}
\begin{*remark}
	Summe \eqref{eq_pred_stoch_process} heissen in der Analysis \person{Riemann}-\person{Stieltjes}-Summen. Werden für Konstruktionen des RS-Integrals $\int h \d \rho$ verwendet.
\end{*remark}
\begin{*definition}[lokal beschränkt]
	Ein stochastischer Prozess $(H_n)_{n \in \N}$ heißt \begriff{lokal beschränkt}, wenn eine (definierte) Folge $c_ \in \R_{\ge 0}$ existiert, sodass
	\begin{align*}
		\abs{H_n} \le c_n \text{ f.s. } \quad \forall n \in \N
	\end{align*}
\end{*definition}
\begin{theorem}
	Sei $X$ adaptiert stochastischer Prozess (bezüglich Filtration $(\F_n)_{n \in \N}$). Dann sind äquivalent:
	\begin{enumerate}
		\item $X$ ist Martingal
		\item $(H \bigcdot X)$ ist Martingale für alle lokal beschränkten, vorhersehbaren $(H_n)_{n \in N}$
	\end{enumerate}
	Das heisst: stochastische Integral erhält die Martingal-Eigenschaft.
\end{theorem}
\begin{proof}
	8.11.2019!
\end{proof}
\begin{*remark}
	Die ZV $H$ wird später die Analagestrategie sein.
\end{*remark}