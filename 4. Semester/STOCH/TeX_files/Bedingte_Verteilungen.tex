In \cref{3_1_3}: $(\O, \F, \P), A,B \in \F$ %TODO add reference!
\begin{align*}
	\P(A \mid B) = \begin{cases}
	\frac{\P(A \cap B)}{\P(B)} &\quad \P(B) > 0\\
	0/ \text{ beliebig} &\quad \P(B) = 0
	\end{cases}
\end{align*}
In Fall $\P(B) > 0$ ist $\P(\ast\mid B)$ ein Wahrscheinlichkeitsmaß und wir können das Integral 
\[
	\E[X \mid B] := \int X(\omega) \P(\d \omega \mid B)
\]
definieren. Wir bezeichnen die als \begriff{bedingten Erwartungswert} von $X$. Für $X= \indi_{A}$ folgt (für $\P(B) > 0$)
\[
	\int X(\omega) \P(\d \omega \mid B) = \P (A \mid B) = \frac{\P(A \cap B)}{\P(B)} = \frac{\E[\indi_{A\cap B}]}{\P(B)} = \frac{\E[X \indi_{B}]}{\P(B)}
\]
und mittels maßtheoretischer Induktion folgt
\begin{align*}
	\E[X \mid B] = \frac{\E[X \indi_B]}{\P(B)}
\end{align*}
allgemein ($X \in \Ln{1}, \P(B) > 0$)
\begin{enumerate}[label=]
	\item \ul{Frage:} (Wie) können wir bedingte Erwartungswerte definieren, wenn $\P(B) = 0$?
\end{enumerate}
\section{Bedingte Verteilungen}
Seien $X,Y$ Zufallsvariablen gegeben durch
\begin{align*}
	X: (\O,\F) \to (\O_X, \F_X)\\
	Y: (\O,\F) \to (\O_Y, \F_Y)
\end{align*}
\begin{itemize}
	\item Falls $\O_Y$ höchstens abzählbar ist, gilt ($\nearrow$ \propref{sec:3})
	\[
		\P(X \in A \mid Y = y) = \begin{cases}
		\frac{\P(X\in A, Y = y)}{\P(Y = y)} &\quad \P(Y=y) > 0\\
		0/ \sonst & \quad \P(Y=y) = 0
		\end{cases}
	\]
	Insbesondere folgt mit dem Satz der totalen Wahrscheinlichkeit
	\begin{align*}
		\P(X\in A, Y\in B) &= \sum_{y\in B} \P(X \in A \mid Y = y)\P(Y=y) \quad \forall B \in \F_Y\\
		&= \int_B \P(X \in A \mid Y=y) \P_Y(\d y) \label{eq:6:1_1} \tag{$\star$}
	\end{align*}
	\item \ul{Idee:} Verwende \eqref{eq:6:1_1} um bedingte Verteilung zu definieren!\\
	Sei also $\mu_A : \O_Y \to \R$ gegeben so dass
	\begin{align*}
		\P(X \in A, Y \in B) = \int_B \mu_A(y)\P_Y (\d y) \quad \forall B \in \F_Y \label{eq:6:1_2} \tag{$\star\star$}
	\end{align*}
	Da $\O_Y$ abzählbar, gilt $\set{y} \in \F_Y \quad \forall y \in \O_Y$. Also folgt aus \eqref{eq:6:1_2}, dass
	\begin{align*}
		\P(X \in A, Y=y) &= \int_{\set{y}} \mu_A (y) \P_Y(\d y)\\
		&= \mu_A (y) \P_Y(Y=y)
	\end{align*}
	Falls $\P(Y=y)\neq 0$ folgt sofort
	\begin{align*}
		\mu_A(y) = \frac{\P(X \in A, Y=y)}{\P(Y=y)} = \P(X\in A \mid Y = y)
	\end{align*}
	Andererseits gilt
	\begin{align*}
			\P_Y (\set{y \in \O_Y: \P(Y=y)= 0}) = \sum_{\substack{y\in \O_Y\\ \P(Y=y) = 0}} \P_Y(\set{y}) = 0\\
			\intertext{so dass}
			\mu_A(y) = \P(X \in A\mid Y = y) \quad \P_Y \text{ f.s} \text{ (d.h. bis auf $\P_Y$-Nullmengen)}
			\intertext{bzw.}
			\P_Y(\set{y \colon \mu_A(y) \neq \P(X \in A \mid Y=y)}) = 0
	\end{align*}
	\item Falls $\O_Y$ überabzählbar ist und $y \in \O_Y$ mit $\P(Y=y) = 0$ (z.B. $Y$ hat Dichte). Wir werden sehen, dann existiert $\mu_A: (\O_Y, \F_Y) \to (\R_+, \borel(\R_+))$ messbar, so dass \eqref{eq:6:1_2} gilt. Insbesondere ist $\mu_A$ dann bis auf $\P_Y$-Nullmengen eindeutig bestimmt und wir können definieren:
	\[
		\P(X\in A\mid Y=y) = \mu_A(y)
	\]
\end{itemize}
Wir benötigen:
\begin{proposition}[\person{Radon}-\person{Nikodym} für endliche Maße]
	\proplbl{6_1_1:Radon}
	Seien $\mu, \nu$ zwei endliche Maße auf $(\O,\F)$. Dann ist $\nu$ absolut stetig bzgl. $\mu$ $(\nu \ll \mu)$ genau dann wenn $\nu$ eine messbare Dichte $f$ bezüglich $\mu$ besitzt, d.h. wenn
	\[
		\nu(A) = \int_A f(\omega)\d \mu(\omega)  \quad \forall A \in \F.
	\]
	Insbesondere ist $f$ $\mu$-f.ü. eindeutig bestimmt.
\end{proposition}
\begin{proof}
	$\nearrow$ MINT Schilling Satz 19.2.
\end{proof}
\begin{conclusion}
	Sei $(\O,\F,\P)$ Wahrscheinlichkeitsraum und
	\begin{align*}
		X: (\O,\F) \to (\O_X, \F_X)\\
		Y: (\O, \F) \to (\O_Y, \F_Y)
	\end{align*}
	Zufallsvariablen. Sei $A \in \F_X$ beliebig. Dann existiert $\mu_A: (\O_Y, \F_Y) \to ([0,1], \borel([0,1]))$ messbar, so dass
	\[
		\P(X\in A, Y \in B) = \int_B \mu_A(y)\P_Y(\d y) \quad \forall B \in \F_Y.
	\]
	Wir nennen $\P(X \in A\mid Y = y)$ \begriff{bedingte Wahrscheinlichkeit}.
\end{conclusion}
\begin{proof}
	Offensichtlich impliziert $\P(y \in B) = 0$ auch $\P(X\in A, Y \in B) = 0$ so dass
	\begin{align*}
		\P(X \in A, Y \in \cdot) \ll \P(Y \in \cdot) = \P_Y(\cdot)
	\end{align*}
	Nach \propref{6_1_1:Radon} existiert eine messbare Funktion $f: (\O_Y, \F_Y) \to (\R_+, \borel(\R_+))$ mit
	\begin{align*}
		\P(X \in A, Y \in B) = \int_B f(y)\P_Y(\d y) \quad \forall B \in \F_Y.
	\end{align*}
	Sei $D =\set{y : f(y) > 1}$, dann gilt zudem $\P_Y(D) = 0$, denn
	\begin{align*}
		\P(Y \in D) \ge \P(X \in A, Y \in D) = \int_D f(y)\P_Y(\d y)
		\intertext{impliziert}
		0 \ge \P(X \in A, Y \in D) - \P(Y \in D) = \int_D (\underbrace{f(y) - 1}_{> 0 \text{ in }D})\P_Y (\d y) \ge 0
	\end{align*}
	also gilt $\P(D) = 0$. Setze also
	\begin{align*}
		\mu_A (y) := \begin{cases}
			f(y) &\quad y \in D^C\\
			0 &\quad y \in D,
		\end{cases}
	\end{align*}
	dann erfüllt $\mu_A$ allen Eigenschaften.
\end{proof}
Für fixiertes $A \in \F_X$ ist die nun definierte bedingte Wahrscheinlichkeit eindeutig bis auf $\P_Y$-Nullmengen. Für fixiertes $y$ (und $A$ variierend) ist $\P(X \in A \mid Y = y)$ aber nicht immer ein Wahrscheinlichkeitsmaß!
\begin{example}
	\proplbl{6_1_3}
	Betrachte den Wahrscheinlichkeitsraum $([0,1],\borel([0,1]), \Gleich([0,1])$ und die Zufallsvariablen $X,Y$ mit $X(\omega) = Y(\omega) = \omega \quad \forall \omega \in [0,1]$. Dann
	\begin{align*}
		\P(X \in A, Y \in B) = \P(Y \in A \cap B) = \int_B\indi_{A}(\omega)\P_Y(\d \omega) \quad \forall B \in \F
	\end{align*}
	so dass $\indi_{A}(\omega)$ eine Version von $\P(X \in A \mid Y = y)$. Insbesondere ist $\indi_{A} (y)$ für jedes $y \in [0,1]$ ein Wahrscheinlichkeitsmaß. Setzen wir
	\begin{align*}
		f(A) = \begin{cases}
		\sup A &\quad A \neq \emptyset\\
		0 &\quad A = \emptyset
		\end{cases}
		\intertext{so ist auch}
		\P'(X \in A \mid Y = y) = \indi_{A} (y)+ \indi_{\set{f(y)}}(y)
	\end{align*} 
	eine Version der bedingten Wahrscheinlichkeit, denn
	\begin{align*}
		\int_B (\indi_{A} (\omega) + \indi_{\set{f(y)}} (\omega)) &= \P(Y \in A \cap B) + \underbrace{\cancel{\P(Y \in B \cap f(A))}}_{=0}\\
		&= P(X \in A, Y \in B)
	\end{align*}
	Allerdings ist $\P'(X \in \cdot \mid Y = y)$ kein Wahrscheinlichkeitsmaß, denn für beliebiges $y \in [0,1]$ gilt
	\begin{align*}
		\P'(X\in [0,y]\mid Y = y) = \indi_{[0,y]} + \indi_{f([0,y])} = 2
	\end{align*}
\end{example}
Wie können solche Maße wie im \propref{6_1_3} ausgeschlossen werden? Dadurch ist folgende Definition motiviert.
\begin{definition}[reguläre bedingte Verteilung]
	\proplbl{6_1_4}
	Eine bedingte Verteilung $\P(X \in \cdot\mid Y = \cdot)$ heißt \begriff{regulär}, wenn $\P(X \in \cdot \mid Y =y)$ für alle $y \in \O_Y$ ein Wahrscheinlichkeitsmaß ist.
\end{definition}
Die Existenz regulärer bedingter Verteilungen ist nicht trivial. Wir beschränken uns daher auf den reellen Fall.
\begin{proposition}
	\proplbl{6_1_5}
	Sei $(\O, \F, \P)$ Wahrscheinlichkeitsraum, $X: (\O, \F) \to (\Rd, \borel(\Rd))$, 
	$Y: (\O, \F) \to (\O_Y, \F_Y)$ Zufallsvariablen. Dann existiert eine reguläre bedingte Verteilung
	\[
		\P(X \in \cdot \mid Y = \cdot).
	\]
\end{proposition}
\begin{proof}
	Nur Beweisskizze (ausführlich wird ergänzt!)\\
	Sei $\tilde{\P}(X \in A \mid Y = \cdot)$ eine beliebige Version der bedingten Verteilung.
	\begin{itemize}
		\item \ul{Idee:} ``Korrigiere'' $\tilde{\P}$ auf geeigneten $\P_Y$-Nullmengen um geeigneter reguläre Version zu erhalten.
		\begin{enumerate}
			\item Es gibt eine $\P_Y$-Nullmenge $N_1$, so dass
			\begin{align*}
				\tilde{\P}(X \in \Rd \mid Y = y) = 1 \quad \forall y \notin \N_1
			\end{align*}
			\item Definiere $\G^d := \set{\bigcup_{i = 1}^k [a_, b_i] \mid a_i, b_i \in \G^d, k \in \N}$. Dann gibt es eine $\P_Y$-Nullmenge $N_2$, so dass $\tilde{\P}(X \in \cdot \mid Y = y)$ nicht negativ und additiv auf $\G^d$ für $Y \notin \N_2$.
			\item Es gibt eine $\P_Y$-Nullmenge $N_3$, so dass $\tilde{\P}(X \in \cdot \mid Y = y)$ für alle $y \notin N_2 \cup N_3$ $\sigma$-additiv auf $\G^d$ ist.
			\item Sei $N = N_1 \cup N_2 \cup N_3$ $\P_Y$-Nullmengen. Für $y \in N^C$ existiert eine Erweiterung von $\tilde{\P}(X \in \cdot \mid Y = y)$ zu einem Wahrscheinlichkeitsmaß $\hat{\P}(X \in \cdot \mid Y = y)$ auf $\borel(\Rd)$. Definiere
			\begin{align*}
				\P(X \in \cdot \mid Y = y) = \begin{cases}
				\hat{\P}(X \in \cdot \mid Y = y) & \quad y \notin N\\
				\P_0 \text{ beliebiges Wahrscheinlichkeitsmaß } & \quad y \in N
				\end{cases}
			\end{align*} 
			dann ist $\P(Y \in \cdot \mid Y = y)$ ein Wahrscheinlichkeitsmaß.
			\item $\P(X \in \cdot \mid Y = y)$ ist eine Version der bedingten Verteilung.
		\end{enumerate}
	($\nearrow$ befindet sich in einer PDF-Datei auf Opal, eventuell hier hinzufügen später!) %TODO
	\end{itemize}
\end{proof}
\begin{proposition}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $X,Y\colon (\O,\F) \to (\R, \borel(\R))$ Zufallsvariablen mit gemeinsammer Dichte $\rho_{X,Y}(x,y)$. Dann besitzen  $X \und Y$ die \begriff{Randdichten} (``Marginaldichten'')
	\[
		\rho_X (x) = \int_{\R} \rho_{X,Y}(x,y) \d y \quad \rho_Y (y) = \int_{\R} \rho_{X,Y}(x,y) \d x
	\]
	Zudem gilt für alle $A \in \borel(\R), y \in \R$:
	\[
		\P(X \in A \mid Y=y) = \begin{cases}
		\int_A \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} \d x &\quad \rho_Y(y) \neq 0\\
		0 &\quad \sonst
		\end{cases}
	\]
	Insbesondere besitzt $\P(X \in \cdot \mid Y =y)$ für alle $y \in \R\mit \rho_Y(y) = 0$ die Dichte (\begriff{bedingte Dichte})
	\[
		\rho_{X \mid Y} (x,y) = \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} 
	\]
	und ist in diesem Fall ein Wahrscheinlichkeitsmaß.
\end{proposition}
\begin{proof}
	Randdichten:
	\begin{align*} 
		\P(X \in A) &= \P(X \in A, Y \in \R) = \int_{\R}\int_A \rho_{X,Y}(x,y) \d x \d y \\%Reihenfolge egal, nach FUBINI und Tonelli
		&= \int_A \underbrace{\int_{\R} \rho(x,y)\d y}_{\rho_X(x)}\d x
	\end{align*}
	Zudem gilt für $A, B \in \borel(\R)$
	\begin{align*}
		\P(X \in A, Y\in B) &= \int_{\R} \int_A \rho_{X,Y} (x,y) \d x \d y\\
		&= \int_B \int_A \rho_{X,Y} (x,y) \indi_{\rho_Y (y) >0} \d x \d y + \\
		&+\int_B \int_A \rho_{X,Y} (x,y) \indi_{\rho_Y (y) >0} \d x \d y\\
		&= \int_B \int_A \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} \indi_{\rho_Y (y) >0} \d x \rho_Y(y) \d y + \\
		&+\underbrace{\int_{B \cap \rho_Y(y) >0} \underbrace{\int_A \rho_{X,Y} (x,y) \d x}_{=\int_{\R} \rho_{X,Y}(x,y) \d x = \rho_Y(y)} \d y}_{=0}\\
		&= \int_B \int_A \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} \indi_{\rho_Y (y) >0} \d x \P_Y(\d y),
		\end{align*}
		so dass
		\begin{align*}
		\P(X \in A \mid Y = y) = \begin{cases}
		\int_A \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} \d x & \quad \rho_Y(y) >0\\
		0 &\quad \sonst
		\end{cases}
	\end{align*}
	nach Definition der bedingten Verteilung \propref{6_1_4}. Für $y \mit \rho_Y(y) > 0$ folgt
	\begin{align*}
		\P(X \in \R \mid Y = y) = \int_{\R} \frac{\rho_{X,Y}(x,y)}{\rho_Y(y)} \d x = \frac{1}{\rho_Y(y)} \int_{\R} \rho_{X,Y}(x,y) \d x = \frac{\rho_X(x)}{\rho_Y(y)} = 1.
	\end{align*}
\end{proof}
\begin{example}
	\proplbl{6_1_7}
	Betrachte $f(x,y) = cx e^{-x(y+1)} \quad x,y > 0 \quad c \in \R$ bestimmt.
	Damit $f$ die Dichte zweier Zufallsvariablen $X,Y$ ist, muss
	\[
		\int_{\R_+} \int_{\R_+} f(x,y) \d x \d y \over{!}{=} 1
	\]
	gelten. Dazu sei
	\begin{align*}
		\int_0^{\infty} \int_0^{\infty} cx e^{-x(y+1)} \d y \d x &= c \int_0^{\infty} x e^{-x} \int_0^{\infty} e^{-xy} \d y \d x\\
		&= c \int_0^{\infty} x e^{-x} \sqbrackets{- \frac{1}{x}e^{-xy}}_{y=0}^{\infty} \\
		&= c \underbrace{\int_0^{\infty}e^x \d x}_{=1} = c \over{!}{=} 1 
	\end{align*}
	Wähle $x = 1$. Als Randdichten von $X \und Y$ folgen
	\begin{align*}
		f_X(x) &= \int_0^{\infty} f(x,y) \d y = \int_0^{\infty} x e^{-x(y+1)} \d y\\
		&\over{\text{s.o.}}{=} x^{-x} \quad x >0 
	\end{align*}
	(d.h. $X \sim \EXP(1)$) und 
	\begin{align*}
		f_Y(y) &= \int_0^{\infty} f(x,y) \d x = \int_0^{\infty} x e^{-x(y+1)}\d x\\
		\over{\text{P.I.}}&{=} \sqbrackets{x \frac{-1}{y+1} e^{-x(y+1)}}_{x=0}^{\infty} + \frac{1}{y+1} \int_0^{\infty} e^{-x(y+1)}\d x\\
		&= 0 + \frac{1}{y+1} \sqbrackets{\frac{-1}{y+1}e^{-x(y+1)}}_{x=0}^{\infty}\\
		&= \frac{1}{(y+1)^2} \quad y > 0
	\end{align*}
	(Dies ist die Dichte einer \person{Pareto}-Verteilung.) Insbesondere sind $X \und Y$ nicht unabhängig, da sonst $f(x,y) = f_X(x)f_Y(y)$ gelten müsste. Die bedingte Dichte von $X$ gegeben $Y = y_0, y_0 >0$ berechnet sich als
	\begin{align*}
	f_{X\mid Y=y_0} (x) = \frac{f(x,y_0)}{f_Y(y_0)} = x(y_0 + 1)^2 e^{-x(y_0+1)}
	\end{align*}
	und dies ist die Dichte einer $\Gam(y_0 +1,2)$-Verteilung, d.h.
	\[
		X \mid Y = y_0 \sim \Gam(y_0 +1, 2).
	\]
\end{example}