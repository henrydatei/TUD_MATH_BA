\begin{enumerate}
	\item \emph{Frage:} \\
	\cref{4_5} Durchschnittliche Wartezeit? $\rightsquigarrow$ Erwartungswert\\
	Wie stark ist die Streuung um den Durchschnitt? $\rightsquigarrow$ Varianz
\end{enumerate}
\section{Der Erwartungswert}
\begin{definition}[Erwartungswert]
	\proplbl{5_1}
	Sei $(\O, \F, \P)$ Wahrscheinlichkeitsraum und $X: (\O, \F) \to (\R, \borel(\R))$ Zufallsvariable. Dann ist
	\begin{align*}
		\E[X] = \int_{\O} X(\omega) \P(\d \omega) = \int_{\R} x \P(X  \in \d x)
	\end{align*}
	der \begriff{Erwartungswert von $X$}.
\end{definition}
\begin{*hint}
	Der Erwartungswert von $X$ existiert, genau dann wenn
	\begin{align*}
		\int_{\O} \abs{X(\omega)}\P(\d \omega) < \infty \bzw \E[\abs{X}] < \infty
	\end{align*}
	d.h. genau dann wenn $X \in \Ln{1} (\P)$.\\
	Für nichtnegative Zufallsvariablen ist der Erwartungswert immer definiert, wenn wir $+\infty$ als zulässigen Wert annehmen, was wir in der Folge auch tun.
\end{*hint}
\begin{example}
	\proplbl{5_2}
	$(\O, \F, \P)$ Wahrscheinlichkeitsraum, $A \in \F$ und sei $X: (\O, \F) \to (\R, \borel(\R))$ die Indikatorvariable
	\[
		X(\omega) = \indi_{A} (\omega)
	\]
	Dann gilt: $X \in \Ln{1}(\P)$ und
	\begin{align*}
		\E[X] = \int_{\O} \indi_{A} (\omega) \P(\d \omega) = \int_{A} \P(\d \omega) = \P(A).
	\end{align*}
\end{example}
\begin{proposition}
	\proplbl{5_3}
	Sei $X: (\O, \F) \to (\Rn, \borel(\Rn)$ Zufallsvariable und $f: \Rn \to \R$ Borel-messbar. Dann
	\begin{align*}
		\E[f(X)] = \int f(X)\d \P = \int_{\O} \E(X(\omega)) \d \P(\omega) = \int_{\Rn} f(X) \P(X \in \d x). 
	\end{align*}
\end{proposition}
\begin{proof}
	Sei $f(X)$ eine reelle Zufallsvariable. Die Formel folgt direkt auf dem Transformationssatz für Bildmaße ($\nearrow$ Schilling MINT 18.1).
\end{proof}
\begin{proposition}[Erwartungswerte bei Existenz einer (Zähl-)dichte]
	\proplbl{5_4}
	Sei $X: (\O, \F) \to (\Rn, \borel(\Rn))$ Zufallsvariable und
	\begin{align*}
		f: \Rn \to \R \text{ Borel-messbar}.
	\end{align*}
	\begin{enumerate}
		\item \emph{diskreter Fall:} Ist $\P_X$ ein Wahrscheinlichkeitsmaß auf $(\Z, \pows(\Z))$ und der Zähldichte $\rho$, so
		\begin{align*}
			\E[f(X)] = \sum_{x \in \Z} f(x)\rho(x).
		\end{align*}
		\item \ul{stetiger Fall:} Besitzt $\P_X$ eine Dichte $\rho$ (bzgl Lebesguemaß), so
		\begin{align*}
			\E[f(X)] = \int_{\R} f(x)\rho(x) \d x
		\end{align*}
	\end{enumerate}
\end{proposition}
\begin{proof}
	Klar aus \propref{5_1} und \propref{5_3}.
\end{proof}
\begin{example}
	\proplbl{5_5}
	Sei $X \sim \Bin(n,p)$. Dann gilt
	\begin{align*}
		\E[X] &= \sum_{k=1}^n k\binom{n}{k}p^k (1-p)^{n-k}\\
		&= \sum_{k=1}^n \frac{n!}{(n-k)!(k-1)!}p^k (1-p)^{n-k}\\
		&= np \sum_{k=1}^n \underbrace{\binom{n-1}{k-1}p^{k-1}(1-p)^{n-1-(k-1)}}_{\text{Zähldichte }\Bin(n-1,p) \text{ in } k-1}\\
		&= np.
	\end{align*}
\end{example}
Da der Erwartungswert ein Integral ist, übertragen sich viele Eigenschaften. 
\begin{proposition}[Eigenschaften des Erwartungswertes]
	\proplbl{5_6}
	Seien $X,Y,X_n: (\O,\F) \to (\R, \borel(\R)), n \in \N$ Zufallsvariablen in $\Ln{1}(\P)\und a,b \in \R$ konstant.
	\begin{enumerate}
		\item \ul{Linearität:} $\E[aX +bY] = a\E X + b\E Y$
		\item \ul{Monotonie:} $X \le Y$, d.h. $X(\omega) \le Y(\omega), \forall \omega \in \Omega$. Dann gilt 
		\[
			\E[X] \le \E[Y] \text{ und insbesondere gilt } X \ge 0 \implies \E X \ge 0.
		\]
		\item \ul{Lemma von \person{Fatou}:}
		\begin{align*}
			\E[\liminf_{n \to \infty}X_n] \le \liminf_{n \to \infty} \E[X_n]
		\end{align*}
		\item \ul{Satz von \person{Beppo}-\person{Levi}:} Wenn $X_n \ge 0 \und X_n \uparrow X$ so gilt:
		\begin{align*}
			\E[X] = \sup_{n \in \N} \E[X_n] = \lim_{n \to \infty} \E[X_n]
		\end{align*}
		\item \ul{Dominierte Konvergenz/ Satz von \person{Lebesgue}:} Sei $\lim_{n \to \infty} X_n(\omega) = X(\omega)$ und
		\begin{align*}
		\P(\set{\omega: \abs{X_n(\omega)} \le Y(\omega)}) = 1 \qquad (\abs{X} \le Y \P \text{ fast sicher})
		\end{align*}
		für $Y \in \Ln{1}(\P) \implies X \in \Ln{1}(\P) \und \lim_{n \to \infty} \E[X_n] = \E[X]$
		\item \ul{\person{Markov}-Ungleichung:} Sei $\epsilon > 0$, dann gilt
		\begin{align*}
		\P(\abs{X} \ge \epsilon) \le \frac{1}{\epsilon} \E[\abs{X}]
		\end{align*}
		\item \ul{\person{Hölder}-Ungleichung:} Sei $1 \le p,q\le \infty, \frac{1}{p}+ \frac{1}{q} = 1$
		\begin{align*}
		\E[\abs{XY}] \le \brackets{\E[\abs{X}^p]}^{\frac{1}{p}} \brackets{\E[\abs{Y}^q]}^{\frac{1}{q}}
		\end{align*}
		($p =q =2$ \person{Cauchy}-\person{Schwarz}-Ungleichung)
		\item \ul{\person{Jensen}'sche Ungleichung:} Sei $X \ge 0 \und \varphi: [0,\infty) \to [0,\infty)$ konvex, messbar \\
		$\varphi(\E[X]) \le \E[\varphi(X)]$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	$\nearrow$ Schilling MINT.
\end{proof}
\begin{example}
	\proplbl{5_7}
	Da für $X_1,\dots, X_n \sim \Ber(p)$ unabhängig gilt, dass $\underbrace{X_1 + \cdots + X_n}_{=X} \sim \Bin(n,p)$ folgt
	\begin{align*}
	  \E[X] &= \E[X_1 + \cdots + X_n] = \E[X_1] + \cdots + \E[X_n] = n \E[X_1]\\
		&= n (1 \cdot \underbrace{\P(X_1 = 1)}_{=p} + 0 \cdot \P(X_1 = 0))\\
		&= n\cdot p.
	\end{align*}
\end{example}
\begin{proposition}[Produktformel für Erwartungswerte]
	\proplbl{5_8}
	Seien $(\O,\F,\P)$ Wahrscheinlichkeitsraum und $X_1,\dots,X_n: \O \to \R^d$ unabhängige Zufallsvariablen und $f_1,\dots, f_n: \Rd \to \R$ messbar. Wenn $f_i(X_i)\ge 0, i = 1, \dots,n$ sei mit $f_i(X_i) \in \Ln{1}(\P), i=1,\dots,n$, dann gilt
	\[
		\E\sqbrackets{\prod_{i=1}^n f_i(X_i)} = \prod_{i=1}^n \E[f_i(X_i)]
	\]
\end{proposition}
Für den Beweis von \propref{5_8} benötigen wir:
\begin{lemma}
	\proplbl{5_9}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $X,Y: \O \to \Rd$ unabhängige Zufallsvariablen und $h: \R^{2d} \to \R$ messbar. Falls $h \ge 0\oder h(X,Y) \in \Ln{1}(\P)$, dann
	\begin{align*}
		\E[h(X,Y)] &= \int_{\Rd}\int_{\Rd} h(x,y) \P(X \in \d x)\P(Y \in \d y)\\
		&= \E\sqbrackets{\int_{\Rd} h(X,y) \P(Y \in \d y)}\\
		&= \E\sqbrackets{\int_{\Rd} h(x,Y)\P(X \in \d x)}
	\end{align*}
\end{lemma}
\begin{proof}
	Sei $h(X,Y)$ eine reelle Zufallsvariable und
	\begin{align*}
		\E[h(X,Y)] &= \int_{\O} h(X(\omega),Y(\omega))\d \P(\omega)\\
		\over{\propref{5_1_3}}&{=} \int_{\Rd} h(x,y)\P(X\in \d x, Y \in \d y)\\
		\over{X \upmodels Y, \propref{3_2_21}}&{=} \int_{\R^{2d}} h(x,y) \P_X \otimes \P_Y(\d x, \d y)\\
		\over{\text{\person{Fubini}}}&{=} \int_{\Rd} \int_{\Rd} h(x,y)\P_X(\d x)\P_Y(\d y)\\
		\over{\propref{5_1_3}}&{=} \int_{\O} \int_{\Rd} h(x,Y(\omega))\P_X (\d x)\P(\omega)\\
		&= \E[\int_{\Rd} h(x,Y)\P_X(\d x)].
	\end{align*}
\end{proof}
\begin{proof}[\propref{5_8}]
	Betrachte $n=2$, Zufallsvariablen $X,Y$ und Abbildungen $f,g$. Setze $h(x,y) = f(x)g(y)$, dann folgt für $f,g \ge 0$ mit \propref{5_9}
	\begin{align*}
		\E[f(X)g(Y)] &= \int_{\Rd} \int_{\Rd} f(x)g(y)\P(X\in \d x)\P(Y\in \d y)\\
		&= \int_{\Rd} f(x)\P(X \in \d x)\int_{\Rd} g(y)\P(Y \in \d y)\\
		&= \E[f(X)]\cdot \E[g(Y)].
	\end{align*}
	Für $f(X),g(Y) \in \Ln{1}(\P)$ zeigt obige Rechnung
	\begin{align*}
		\E[f(X)g(Y)] = \E[\abs{f(X)}]\E[\abs{g(Y)}] < \infty
	\end{align*}
	also $f(X)g(Y) \in \Ln{1}(\P)$. Die Aussage folgt über die obige Rechnung. Für allgemeines $n$ folgt \propref{5_8} durch Iteration mit \cref{3_19}.
\end{proof}
\begin{tikzpicture}
	\begin{axis}[
	xmin=-5, xmax=5,
	ymin=0, ymax=0.5,
	samples=400,
	axis y line=none,
	axis x line=none,
	ticks=none,
	]
	\addplot+[mark=none, blue] {1/(sqrt(2*pi))*exp(-0.5*x^2)};
	\addplot+[mark=none, blue] {1/(sqrt(2*pi*2))*exp(-0.5*x^2/2)};
	\end{axis}
	
	\draw[->] (0,0) -- (7,0);
	\draw[->] (0,0) -- (0,5);
	\node[lime!80!black] at (3.5,-0.5) (E) {$\mathbb{E}[X]$};
	\draw[lime!80!black] (3.5,-0.1) -- (3.5,0.1);
	\draw [lime!80!black,decorate,decoration={brace,amplitude=10pt,mirror, raise=1pt},yshift=0pt]
	(0,0) -- (3,0) node [lime!80!black,midway,yshift=-0.6cm] {Streuung};
	\node at (-0.5,0) (empty) {};
\end{tikzpicture}